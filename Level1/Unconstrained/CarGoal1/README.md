### Conclusion

The system operates with an average episode length of 1000 steps and achieves an average reward of 10.6. It runs efficiently at 358 frames per second, completing 34 iterations within 284 seconds, accumulating a total of 102,000 timesteps.

Key training metrics highlight stable performance:
- **Approximate KL Divergence:** 0.00496
- **Clip Fraction:** 0.0433 with a clip range of 0.2
- **Entropy Loss:** -2.75
- **Explained Variance:** 0.299
- **Learning Rate:** 0.0001
- **Loss:** -0.0108
- **Policy Gradient Loss:** -0.00692
- **Standard Deviation:** 0.955
- **Value Loss:** 0.0175

Episode returns vary between 9.851 and 15.918, reflecting the system's ability to adapt and perform across episodes.

In summary, the system demonstrates robust and efficient training dynamics with consistent metrics. Further optimization efforts could focus on improving average reward performance and reducing variability through enhanced policy refinement and adaptive learning strategies.


[Screencast from 07-04-2024 08_08_45 PM.webm](https://github.com/Naveed776/Safe_expolration_RL_SafetyGym/assets/91262613/bf7f3083-1840-4b28-90c9-48514bfe2c07)



![Figure_1](https://github.com/Naveed776/Safe_expolration_RL_SafetyGym/assets/91262613/087bef19-5af4-42c9-a7ae-d346a91f9277)

